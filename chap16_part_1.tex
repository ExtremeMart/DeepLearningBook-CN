\chapter{深度学习的结构化概率模型}
\label{chap:16}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% author:YisenWang  %%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% part: 16.0-16.2.5 %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
深度学习借鉴了很多模型形式方面的内容来帮助研究者指导他们的设计理论和算法描述。这其中之一就是有结构的概率模型。我们在前面3.14节的时候，简单的讨论过一些有结构的概率模型。那些简短的介绍已经足够用来理解怎么使用有结构的概率模型来描述第二部分的一些算法。在第三部分，有结构的概率模型是深度学习里很多重要的研究方向的关键成分。为了后续讨论这些研究点，在这一章，我们会详细介绍有结构的概率模型。不过，读者不用担心，这一章是自我完备的，在开始学习这一章之前，读者不需要复习之前的介绍。

有结构的概率模型是描述概率分布的一种方式，它直接通过图的形式来描述概率分布中随机变量之间的关系。这里，我们沿用了图理论里面图的概念，即，节点之间通过边来连接。因为模型的结构是由图来定义的，因此，这些模型也经常被称作图模型。

图模型的研究领域很大，也已经发展了很多不同的模型、训练算法和推断算法。在这一章，我们主要讲一些图模型里最核心的思想，并把重点放在那些对深度学习的领域很有用的一些概念上。如果你已经有很强的图模型背景，你可以跳过本章的大部分内容。但是，即使是一个图模型方面的专家，他也可能会从这一章的最后一部分（16.7节）收益，因为，在16.7节，我们会高亮一些特有的图模型可以用在深度学习里的方法。相比于图模型的研究者，深度学习的参与者更倾向于用非常不同的模型结构、学习算法和推断过程。在这一章，我们指出了他们在偏好上的不同，并解释其中的原因。

在这一章，我们首先描述了建立大规模概率模型的挑战。接着，我们描述了怎么用图来描述一个概率分布的结构。虽然这种方法允许我们克服许多挑战，但它不是没有自己的复杂性。在图模型里，一个最主要的困难是理解在一个图里，哪些变量之间需要直接相连，也就是，对于一个给定的问题，哪种图结构是最合适的。我们在16.5节简介了两种方式来解决这个问题。最后，我们通过讨论深度学习与图模型之间的关系来结束本章。

\section{无结构模型的挑战}
深度学习的目标是延伸机器学习到解决人工智能所面临的各种挑战，这意味着深度学习能够理解具体很丰富结构的高维数据。比如说，我们希望人工智能算法能够理解自然图像，声波表示的语音，以及包含多个单词和标点符号的文档。

分类算法能够从很高维的分布中取出一个输入并用一个类别标签总结他，比如照片里面是个什么物体，语音说的是哪个词，文档是关于哪个话题的。分类的过程忽略了输入里的大部分信息，只产生了一个输出（或那个单一输出值的概率分布）。一个分类器也经常忽略输入的很多部分。比如，当识别图片中一个物体，经常会忽略图片中的背景。

让概率模型做很多其他的任务也是可能的。这些任务通常比分类更昂贵，比如，其中一些可能需要输出多个值，而且大部分要求一个对输入有一个完整的理解，不能忽略其中的任何一部分。这些任务包括：
\begin{itemize}
\item 密度估计：给定一个输入$x$, 机器学习系统返回一个在数据生成分布下的真实密度$p(x)$的估计。这虽然只要求一个输出，但它需要对整个输入有一个完整的理解。即使是输入向量中只有一个元素是不寻常的，该系统也必须给它分配一个很低的概率。
\item 去噪：给定一个损坏的或不正确的观测输入$\hat{x}$，机器学习系统返回一个原始或正确$x$的估计。比如，机器学习系统可能被要求从旧照片去除灰尘或划痕。这需要多个输出（估计的干净样本的每个元素）和对整个输入的理解（因为一个损坏的区域将仍然导致最终的估计是损坏的）。
\item 缺失值的插补：给定$x$的一些元素的观察，要求模型返回在一些或所有未观察到的$x$元素的估计或概率分布。这需要多个输出。因为模型可以被要求恢复$x$的任何元素，它必须理解整个输入
\item 采样：该模型从分布$p(x)$生成新样本。应用包括语音合成，即产生像自然人类语音一样的新波形。这需要多个输出值和整个输入的模型。如果样本甚至有一个元素来自错误的分布，那么抽样过程是错误的。
\end{itemize}

比如一个用很多小的自然图像采样的一个例子，如图16.1所示。

在数千或数百万的随机变量上建模丰富的分布是计算和统计学上的挑战性任务。假设我们只想模拟二进制变量。这是最简单的可能情况，但已经是压倒性的。对于小的$32×32$像素的彩色（RGB）图像，存在$2^3072$个这种形式的可能的二进制图像。这个数字比宇宙中估计的原子数多$10^800$多倍。

一般来说，如果我们希望对一个包含$n$个离散变量且每个变量能取$k$个值的随机向量$x$建模分布，则通过存储具有每个可能结果的一个概率值的查找表来表示$P(x)$的朴素方法需要$k^n$参数！

由以下几个原因导致这样做不可行：
\begin{itemize}
\item 内存：存储表示的成本：对于除了非常小的$n$和$k$之外的所有值，作为表示分布的表将需要太多的值来存储。
\item 统计效率：随着模型中参数数量的增加，使用统计估计器来选择参数值所需的训练数据量也增加。因为基于表的模型具有天文数量的参数，所以需要天文大的训练集来精确拟合。任何这样的模型将在训练集上有非常严重地过拟合，除非作出连接表中差异的附加假设（例如，像在back-off或平滑的n-gram模型中，第12.4.1节）
\item 运行时间：推断成本：假设我们要执行一个推断任务，我们使用联合分布$P(x)$来计算一些其他分布，例如边际分布$P(x_1)$或条件分布$P(X_2|x_1)$。计算这些分布将需要对整个表进行求和，因此这些操作的运行时间与存储模型的不可计算存储成本一样高。
\item 运行时：抽样成本：同样，假设我们要从模型中抽取一个样本。 这样做的天真的方法是抽样一些值$u〜U（0,1$，然后通过表迭代，添加概率值，直到它们超过并返回对应于表中该位置的结果。这需要读取整个表 在最坏的情况下，所以它具有与其他操作相同的指数成本。
\end{itemize}

基于表的方法的问题是我们明确地建模每个可能的变量子集之间可能的交互。我们在实际任务中遇到的概率分布比这更简单。通常，大多数变量仅间接地相互影响。

例如，考虑建模一个团队在接力赛中的完成时间。假设团队由三个跑步者组成：爱丽丝，鲍勃和卡罗尔。在比赛开始的时候，爱丽丝拿着一根接力棒，开始在赛道上跑。在完成她的一圈之后，她将接力棒交给鲍勃。鲍勃然后跑他自己的那一圈，并将指挥棒交给卡罗尔，卡罗尔跑最后一圈。我们可以将他们中的每一个人的完成时间建模为一个连续随机变量。爱丽丝的完成时间不依赖于任何人，因为她走在第一。鲍勃的完成时间取决于爱丽丝的，因为鲍勃没有机会开始跑他的那一圈，直到爱丽丝完成她的那一圈。如果爱丽丝更快，鲍勃会更快地完成，反之亦然。最后，卡罗尔的完成时间取决于她的队友。如果爱丽丝很慢，鲍勃也可能会结束的晚。因此，卡罗尔会有相当的晚的启动时间，因此很可能有一个晚的完成时间。然而，卡罗尔的完成时间仅仅间接地通过鲍勃的完成时间依赖于爱丽丝。如果我们已经知道鲍勃的完成时间，我们将无法通过确定爱丽丝的完成时间来更好地估计卡罗尔的完成时间。这意味着我们可以仅使用两个交互来建模接力赛：爱丽丝对鲍勃的影响和鲍勃对卡罗尔的影响。我们可以省略我们模型中的爱丽丝与卡罗尔的间接相互作用。

结构化概率模型提供了一种用于仅对随机变量之间的直接相互作用进行建模的形式框架。这允许模型具有较少的参数，因此可以从较少的数据可靠地估计。这些较小的模型还显着降低了存储模型，在模型中执行推理，以及从模型中采样的计算成本。

\section{用图来描述模型结构}
结构化概率模型使用图（在图论意义上由边连接的“节点”或“顶点”）来表示随机变量之间的交互。每个节点表示随机变量。每个边表示直接交互。这些直接交互意味着其他的间接交互，但是只有直接交互需要被明确地建模。

通过使用图，有多种方法来描述概率分布中的交互。在下面的章节中，我们描述了一些最受欢迎和有用的方法。图形模型可以大致分为两类：基于有向无环图的模型和基于无向图的模型。

\subsection{有向模型}
一种结构化概率模型是定向图形模型，或者称为置信网络或贝叶斯网络。

定向图形模型被称为“定向”，因为它们的边缘是定向的，也就是说，它们从一个顶点指向另一个顶点。该方向在图中用箭头表示。箭头的方向表示哪个变量的概率分布是根据对方的定义。绘制一个从a到b的箭头，意味着，我们通过条件分布来定义b的概率分布，即a在条件栏的右侧的变量。换句话说，b上的分布取决于a的值。

继续来看第16.1节的接力赛的示例，假设我们命名爱丽丝的完成时间为$t_0$，鲍勃的完成时间为$t_1$，卡罗尔的完成时间为$t_2$。如我们前面所看到的，$t_1$的估计依赖于$t_0$。$t_2$的估计直接依赖于$t_1$但只间接依赖于$t_0$。我们可以在有向图模型中描述这种关系，如图16.2所示。

形式上，由定向无环图定义的变量x定义的有向图形模型。