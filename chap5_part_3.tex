%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% author:KaiserW %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% part:5.7-5.11  %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{监督学习算法}
\label{sec:5.7}
前承\ref{sec:5.1.3}节，有监督学习（Supervised Learning）简单来讲就是一种学习算法，它会学着在某些输入和某些输出之间建立关联，这些输入\textbf{x}和输出\textbf{y}来自于训练集中的样本。很多时候，输出\textbf{y}很难自动采集，而必须由一位人工“监督者”(supervisor)提供，当然即便训练集的拟合目标已经自动采集完成，“监督学习”的名称仍然适用。
\subsection{概率监督学习}
\label{sec.5.7.1}
本书提到的大多数监督学习算法都是基于对概率分布$p(y|x)$的预测。我们可以简单地应用最大似然估计(maximum likelihood estimation)来找到分布$p(y|x;\theta)$参数族的最佳参数向量$\theta$。

已知线性回归(linear regression)对应参数族
\begin{equation}
	p(y|x;\theta) = \mathbb{N} (y;\theta^{T}, \textbf{\textit{I}})
  	\label{form:5.80}
\end{equation}

通过定义不同族的概率分布，我们可以将线性回归推广到分类(classification)情景。如果我们有两个类别，类0和类1，那么接下来只需确定其中一个类的的概率就可以了。类1的概率自然也就决定了类0的概率，因为两个概率值相加必然为1.
基于平均值将实数域上的正态分布进行参数化，这一分布我们也用于线性回归，这里的平均值可以是任意值。但是二元变量的分布则更加复杂一些，因为其平均值必然始终落在0和1之间。一种解决方案是应用逻辑函数（logistic function， 也称sigmoid函数）将线性函数的输出值挤压到(0,1)区间，转换后的值可以理解为是一个概率：
\begin{equation}
	p(y=1|x;\theta) = \sigma (\theta^{T}x)
  	\label{form:5.80}
\end{equation}

这一方法即是逻辑回归(logistic regression)，这名字有些古怪，因为我们实际上用这个模型做分类而不是回归。
对于线性回归，我们可以解正规方程(normal equations)以求得最优权重。而逻辑回归就要复杂一些，它的最优权重没有解析解。我们只能通过最大化对数似然率(log-likelihood)来逼近最优解，具体的策略是，应用梯度下降法(gradient descent)使负对数似然率(negative log-likelihood, NLL)最小化。

这一策略基本可以应用在任何监督学习问题中：对于正确类型的输入/输出变量，写下其条件概率分布的参数族。

\subsection{支持向量机}
\label{sec:5.7.2}

支持向量机(Boser et al., 1992; Cortes and Vapnik, 1995)是最具影响力的监督学习方法之一。该方法与逻辑回归很相似，因为都是由线性函数$\omega^{T}x + b$所驱动。不同于逻辑回归，支持向量机(Support Vector Machine, SVM)并不提供概率值，只有分类结果。当$\omega^{T}x + b$为正，SVM预测为正类；同理当$\omega^{T}x + b$为负，则预测为负类。

支持向量机的关键创新点是\textbf{核技巧}(kernel trick)。核技巧观察到很多机器学习算法可以写作样本的点乘积。例如，支持向量机所用的线性函数可以写作形如
\begin{equation}
	\omega^{T}x + b = b + \sum_{i=1}^{m}{\alpha_{i}x^{T} x^{(i)}} 
    \label{form:5.81}
\end{equation}

这里$x^{(i)}$是一个训练样本，$\alpha$是系数矢量。

以这种方式重写学习算法之后，我们便可以用特征函数$\phi(x)$的输出和函数$k(\textbf{x}, \textbf{x}^{(i)})=\phi(\textbf{x})\cdot\phi(\textbf{x}^{(i)})$替代$\textbf{x}$，其中的$k(\textbf{x}, \textbf{x}^{(i)})=\phi(x)\cdot\phi(\textbf{x}^{(i)})$就叫做\textbf{核}(kernel)。$\cdot$操作符表示与$\phi(x)^{T}\phi(\textbf{x}^{(i)})$类似的内积。在有些特征空间中，我们可能无法使用真正的矢量内积；在有些无限多维的空间中，我们需要使用其他类型的内积，比如基于积分而不是加法的内积。此类内积的完整推导已经超出了本书的范畴。

用核替代了点积之后，我们可以用以下函数做预测
\begin{equation}
	f(x) = b + \sum{i}^{}{\alpha_{i}k(x,x^{(i}}
	\label{form:5.82}
\end{equation}

此函数对$textbf{x}$是非线性的，但是$\phi(\textbf{x})$和$f(\textbf{x}$之间是线性关系。并且$\alpha$和$f(\textbf{x}$也是线性关系。以下过程与基于核的方程都是严格等效的：对所有输入应用$\phi(\textbf{x}$，然后在新的变换空间中学习线性模型。

核技巧的强大有两重原因。首先，它允许我们并使用保证有效收敛的凸优化(convex optimization)技术，把对$x$的非线性函数当作线性的来学习。这是因为我们认为$\phi$是不变的，只优化$\alpha$，换言之，优化算法可以把决策方程在另一个空间中看作线性的。其次，相比于直接构建两个$\phi(\textbf{x})$矢量并显式求点积，核函数$k$的计算效率往往更高。

某些情况下，$\phi(\textbf{x})$甚至可以是无限维的，直接的显式求解将导致无穷的计算消耗。多数情况下，$k(\textbf{x}, \textbf{x'})$是$\textbf{x}$的非线性可解函数，即使$\phi(\textbf{x})$不可解。作为无限维特征空间中可解核的例子，我们构建一个特征映射，从非负整数x到$\phi(\textbf{x})$，设想该映射返回一个包含x个1及无穷多个0的矢量。我们可以写一个核函数$k(\textbf{x}, \textbf{x'}) = min(x, x^{i})$，这与无限维的点积严格等价。

最常用的核是\textbf{高斯核}(Gaussian kernel)
\begin{equation}
	k(u, v) = \mathbb{N}(u-v;0, \sigma^{2}I)
\end{equation}
$\mathbb{N}(x;\mu,\Sigma)$是标准正态密度。这个核也被称为径向基函数(radius basis function, RBF)核，因为其值在$v$空间中沿着$u$向外辐射而减小。高斯核对应着无限维空间里的点积，但是这一空间中的推导不像之前整数核的例子那样直观。

我们可以认为高斯核实现的是一种模板匹配(template matching)。一个与训练标签$y$相关的训练样本$x$构成了类$y$的一个模板。当测试点$x'$与$x$的欧几里得距离(Euclidean distance)很近的时候，高斯核有一个很大的响应，表示$x'$与$x$模板很相似。这一模型给相关训练标签$y$的权重很高。总体上看，预测是把基于对应模板样本(training example)进行过加权的训练标签(training label)组合了起来。

支持向量机并非唯一经由核技巧加强的算法，很多其他的线性模型都可以通过这种方式加强。这类搭载了核的算法也被称作核机器(kernel machine)或核方法(kernel method)。

核机器的最大缺在于，评估决策函数的计算量与训练样本数量呈线性关系，因为第$i$个样本向决策函数提供了$\alpha_{i}k(x,x^{(i)})$。支持向量机可以通过学习一个主要由0构成的矢量$alpha$来缓解这一弊端，对一个新样本做分类，只需要评估$\alpha_{i}\neq0$的样本，这些训练样本就是\textbf{支持向量}(support vector)。

核机器面临的另一大困难就是处理大数据时的超高计算资源消耗，我们将在\ref{sec:5.9}节重新审视该问题。普通的核机器很难提高适用性，我们将在\ref{sec:5.11}节重点讨论。现代深度学习的诞生正是为了突破这些局限，而当下的深度学习“复兴”正是始自Hinton et al(2006)展现了在MNIST数据集上，神经网络比RBF核支持向量机表现的更有力。

\subsection{其他简单的监督学习算法}
\label{sec:5.7.3}

我们已经简单了解过另一非概率的(non-probabilistic)监督学习算法，\textbf{近邻回归}(nearest neighbor regression)。更一般地来讲，k近邻（k-nearest neighbors)是一系列可用于分类和回归的技术。作为无参数学习算法，k近邻不为固定的参数量所限。我们通常认为k近邻算法没有任何参数，而是对训练数据施加了一个简单的函数。实际上k近邻甚至没有真正的训练或学习过程，在测试过程中，当我们想要对一个新测试输入$x$产生新输出$y$的时候，我们直接从训练数据$X$里找到离$x$最近的点，然后返回训练集对应$y$的平均值。在一个监督学习算法里，只要我们能定义出$y$的平均值，这个方法就是好用的。

在分类问题中，我们可以对one-hot编码矢量$\textbf{c}$做平均，其中$c_{y}=1$且其他i值的$c_{i}=0$。对于one-hot编码的平均可以理解为关于类的概率分布。作为无参数学习算法，k近邻的性能属于非常高的。比如当我们有一个多分类任务，并且用0-1的损失值衡量分类表现的时候，随着样本数量趋向无穷多，1近邻(k=1)将收敛至2倍贝叶斯误差(Bayes error)。超过贝叶斯误差的部分是因为，当存在两个距离相同的近邻时，我们只能随机选择其中一个，如此就割裂了两个点之间的关联。如果训练数据无穷多，每个测试点x周围都有无穷多个训练集近邻与之0距离。若是让算法纳入所有这些近邻点而不是随机选取的话，最终就能收敛至贝叶斯误差。当训练集很大的情况下，k近邻的高性能特性使其能够达到极高精度。但是随之而来的计算消耗也是巨大的，且如果训练集较小，预测的泛用性会很差。

k近邻算法的一大缺点就是，它不能自主发现有的特征比其他特征的判别力度更强。想象我们在有$x\subset\mathbb{R}^100$空间中有一个回归任务，x来自于各向同性的高斯分布，但是只有1个变量$x_{1}$与输出有关。更进一步地，假设这一特征变量直接决定输出，比如$y=x_{1}$恒成立。但近邻回归却无法探索出这一简单的模式，多数点$x$的最近邻判别仍将受到$x_{2}$到$x_{100}$的众多特征影响，而不是仅有$x_{1}$单独决定，于是小规模训练集的输出基本会是个随机变量。

\section{非监督学习算法}
\label{sec:5.8}

\section{随机梯度下降法}
\label{sec:5.9}

\section{构建机器学习算法}
\label{sec:5.10}

\section{深度学习算法的动力}
\label{sec:5.11}
